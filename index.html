<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Nikolaos Tsagkas </title> <meta name="author" content="Nikolaos Tsagkas"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?446b1b924650964ebd037c58aeb93688"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tsagkas.github.io/"> </head> <body class="fixed-top-nav "> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="hover-name">Nikolaos Tsagkas</span> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <div class="image-container"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_2_25-480.webp 480w,/assets/img/prof_pic_2_25-800.webp 800w,/assets/img/prof_pic_2_25-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic_2_25.jpg?015b3dba3960895d7fcafc71515ae4d5" class="img-fluid z-depth-1 rounded profile-image" width="100%" height="auto" alt="prof_pic_2_25.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="hover-image"></div> </div> </div> <div class="clearfix"> <p>I am a PhD candidate at the <a href="http://web.inf.ed.ac.uk/ipab" rel="external nofollow noopener" target="_blank">School of Informatics, University of Edinburgh</a>, supported by the <a href="https://www.edinburgh-robotics.org/" rel="external nofollow noopener" target="_blank">Edinburgh Centre for Robotics</a>. I am fortunate to be advised by <a href="https://christopherlu.github.io/" rel="external nofollow noopener" target="_blank">Prof. Chris Xiaoxuan Lu</a> (UCL) and <a href="https://homepages.inf.ed.ac.uk/omacaod/" rel="external nofollow noopener" target="_blank">Prof. Oisin Mac Aodha</a> (UoE). My research focuses on leveraging pre-trained visual representations for robot learning.</p> <p>Prior to my PhD, I earned an MSc in Artificial Intelligence with distinction in 2021 at the University of Edinburgh, under the supervision of <a href="https://homepages.inf.ed.ac.uk/ckiw/" rel="external nofollow noopener" target="_blank">Prof. Chris Williams</a>, where I worked on inference and learning for <a href="https://arxiv.org/abs/2209.03115" rel="external nofollow noopener" target="_blank">generative capsule models</a>. Before that, I spent a year as a Data Scientist at <em>Ernst &amp; Young</em>. I hold a BSc and MSc in Electrical &amp; Computer Engineering (2019) from the University of Patras, Greece, where I researched <a href="https://ieeexplore.ieee.org/document/8900709" rel="external nofollow noopener" target="_blank">real-time hand-gesture recognition</a> using sEMG signals under the guidance of <a href="http://www.ece.upatras.gr/skodras/" rel="external nofollow noopener" target="_blank">Prof. A. Skodras</a>.</p> <div class="news-section"> ğŸ“¢ <b>Latest News:</b> <ul> <li> <b>June 2025</b>: ğŸ† A short version of <a href="../pvrobo/" target="_blank">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning has been accepted</a> <b>for oral and poster presentation</b> at the <a href="https://embodied-ai.org/cvpr2025/" target="_blank" rel="external nofollow noopener">6th Embodied AI Workshop</a> at CVPR 2025. See you in Nashville, Tennessee! ğŸ‘‹ </li> <li> <b>May 2025</b>: ğŸ“œ New paper published on ArXiv! You can access it here: <a href="https://arxiv.org/abs/2505.01179" target="_blank" rel="external nofollow noopener">Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings</a>. </li> </ul> <details> <summary style="font-weight: normal; "> View older news</summary> <ul> <li> <b>Feb 2025</b>: ğŸ“œ New paper published on ArXiv! You can access it here: <a href="https://arxiv.org/abs/2502.03270" target="_blank" rel="external nofollow noopener">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning</a>. </li> <div style="text-align: center; position: relative; padding-bottom: 10px;"> <span style="position: absolute; top: -10px; left: 50%; transform: translateX(-50%); background-color: #000000; color: #ffffff; padding: 0 5px;font-weight: bold;">2024</span> <hr style="border: 1px solid #000;"> </div> <li> <b>Oct 2024</b>: ğŸ—£ï¸ I will be presenting our paper, <a href="https://tsagkas.github.io/click2grasp/">Click to Grasp</a>, during the Robot Vision IV session from 09:00 to 10:00 on Fri 18 Oct at Room 4, IROS'24 in Abu Dhabi, UAE. </li> <li> <b>Sep 2024</b>: ğŸ—£ï¸ Our paper, <a href="https://tsagkas.github.io/click2grasp/">Click to Grasp</a>, will be presented at the <a href="https://www.bmva.org/meetings/24-10-30-RobotWorldModels.html" target="_blank" rel="external nofollow noopener">BMVA Symposium: Robotics Foundation &amp; World Models</a> in London. </li> <li> <b>Aug 2024</b>: ğŸ“œ New paper published on ArXiv! You can access it here: <a href="https://arxiv.org/abs/2408.10123" target="_blank" rel="external nofollow noopener">Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</a>. </li> <li> <b>Jun 2024</b>: ğŸ† Our paper, <a href="https://tsagkas.github.io/click2grasp/" target="_blank">Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors</a>, has been accepted at the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </li> <div style="text-align: center; position: relative; padding-bottom: 10px;"> <span style="position: absolute; top: -10px; left: 50%; transform: translateX(-50%); background-color: #000000; color: #ffffff; padding: 0 5px;font-weight: bold;">2023</span> <hr style="border: 1px solid #000;"> </div> <li> <b>May 2023</b>: ğŸ† Our paper, <a href="https://tsagkas.github.io/vl-fields/" target="_blank">VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations</a>, has been accepted (spotlight) at the <i>Workshop on Effective Representations, Abstractions, and Priors for Robot Learning</i>, ICRA 2023. </li> </ul> </details> </div> <style>.icon{color:#000;font-size:27px}.news-section{background-color:#fafafa;border-left:4px solid#020202;padding:10px 15px;margin:20px 0;border-radius:5px;font-size:16px;box-shadow:3px 3px 10px rgba(0,0,0,0.15)}.news-section summary{font-weight:bold;cursor:grab;color:#111}.news-section ul{padding-left:20px;margin-top:10px}.news-section li{margin-bottom:5px}</style> <p><a href="mailto:n.tsagkas@ed.ac.uk"><i class="fas fa-envelope icon"></i></a> <a href="https://github.com/tsagkas" rel="external nofollow noopener" target="_blank"><i class="fab fa-github icon"></i></a> <a href="https://scholar.google.com/citations?user=cZgkD_oAAAAJ" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar-square ai-24x icon"></i></a> <a href="https://twitter.com/NikolasTsagkas" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter icon"></i></a> <a href="https://www.linkedin.com/in/nikolas-tsagkas/" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin icon"></i></a></p> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zipper_16fps_240-480.webp 480w,/assets/img/publication_preview/zipper_16fps_240-800.webp 800w,/assets/img/publication_preview/zipper_16fps_240-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/zipper_16fps_240.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zipper_16fps_240.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="sochopoulos2025fastflowbasedvisuomotorpolicies" class="col-sm-8"> <div class="title">Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings</div> <div class="author"> Andreas Sochopoulos ,Â Nikolay Malkin ,Â <b>Nikolaos</b> <b>Tsagkas</b> ,Â JoÃ£o Moura ,Â Michael Gienger ,Â andÂ Sethu Vijayakumar </div> <div class="periodical"> <em>In ArXiv Preprint</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2505.01179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ansocho.github.io/cot-policy/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/ansocho/cot_policy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pvrobo_thumbnail_distractors_short-480.webp 480w,/assets/img/publication_preview/pvrobo_thumbnail_distractors_short-800.webp 800w,/assets/img/publication_preview/pvrobo_thumbnail_distractors_short-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/pvrobo_thumbnail_distractors_short.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pvrobo_thumbnail_distractors_short.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tsagkas2025pretrainedvisualrepresentationsfall" class="col-sm-8"> <div class="title">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning <mark>[Oral EAI@CVPR25]</mark> </div> <div class="author"> <b>Nikolaos</b> <b>Tsagkas</b> ,Â Andreas Sochopoulos ,Â Duolikun Danier ,Â Chris Xiaoxuan Lu ,Â andÂ Oisin Mac Aodha </div> <div class="periodical"> <em>In ArXiv Preprint</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2502.03270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/pvrobo/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://github.com/tsagkas/pvrobo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/affordance_thumbnail_150-480.webp 480w,/assets/img/publication_preview/affordance_thumbnail_150-800.webp 800w,/assets/img/publication_preview/affordance_thumbnail_150-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/affordance_thumbnail_150.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="affordance_thumbnail_150.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="li2024learningpreciseaffordancesegocentric" class="col-sm-8"> <div class="title">Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</div> <div class="author"> Gen Li ,Â <b>Nikolaos</b> <b>Tsagkas</b> ,Â Jifei Song ,Â Ruaridh Mon-Williams ,Â Sethu Vijayakumar ,Â Kun Shao ,Â andÂ Laura Sevilla-Lara </div> <div class="periodical"> <em>In ArXiv Preprint</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2408.10123" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/affgrasp/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/c2g_thumbnail_150-480.webp 480w,/assets/img/publication_preview/c2g_thumbnail_150-800.webp 800w,/assets/img/publication_preview/c2g_thumbnail_150-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/c2g_thumbnail_150.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="c2g_thumbnail_150.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tsagkas2024click" class="col-sm-8"> <div class="title">Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors <mark>[Oral Pitch + Poster]</mark> </div> <div class="author"> <b>Nikolaos</b> <b>Tsagkas</b> ,Â Jack Rome ,Â Subramanian Ramamoorthy ,Â Oisin Mac Aodha ,Â andÂ Chris Xiaoxuan Lu </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Abu Dhabi, UAE</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.14526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/click2grasp/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://github.com/tsagkas/click2grasp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vl-fields_thumbnail-480.webp 480w,/assets/img/publication_preview/vl-fields_thumbnail-800.webp 800w,/assets/img/publication_preview/vl-fields_thumbnail-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/vl-fields_thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vl-fields_thumbnail.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tsagkas2023vlfields" class="col-sm-8"> <div class="title">VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations <mark>[Spotlight Talk + Poster]</mark> </div> <div class="author"> <b>Nikolaos</b> <b>Tsagkas</b> ,Â Oisin Mac Aodha ,Â andÂ Chris Xiaoxuan Lu </div> <div class="periodical"> <em>In ICRA Workshops, London, UK</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.12427" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/vl-fields/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecgm_thumbnail-480.webp 480w,/assets/img/publication_preview/ecgm_thumbnail-800.webp 800w,/assets/img/publication_preview/ecgm_thumbnail-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ecgm_thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecgm_thumbnail.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="https://doi.org/10.48550/arxiv.2209.03115" class="col-sm-8"> <div class="title">Inference and Learning for Generative Capsule Models</div> <div class="author"> Alfredo Nazabal* ,Â <b>Nikolaos</b> <b>Tsagkas*</b> ,Â andÂ Christopher K. I. Williams </div> <div class="periodical"> <em>Neural Computation</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2209.03115" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/tsagkas/capsules" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/semg_thumbnail-480.webp 480w,/assets/img/publication_preview/semg_thumbnail-800.webp 800w,/assets/img/publication_preview/semg_thumbnail-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/semg_thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="semg_thumbnail.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="8900709" class="col-sm-8"> <div class="title">On the Use of Deeper CNNs in Hand Gesture Recognition Based on sEMG Signals <mark>[Oral Presentation]</mark> </div> <div class="author"> <b>Nikolaos</b> <b>Tsagkas</b> ,Â Panagiotis Tsinganos ,Â andÂ Athanassios Skodras </div> <div class="periodical"> <em>In 10th International Conference on Information, Intelligence, Systems and Applications (IISA), Patras, Greece</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/document/8900709" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/tsagkas/sEMG-HandGestureRecognition" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> </div> <div class="contact-note"></div> </div> </article> </div> <style>.profile{position:relative;display:inline-block}.image-container{position:relative}.profile-image{transition:opacity .3s ease;position:relative;z-index:1}.profile:hover .profile-image{opacity:0}.hover-image{background-image:url('assets/img/20241024_232955.jpg');background-size:cover;background-position:center;position:absolute;top:0;left:0;right:0;bottom:0;opacity:0;transition:opacity .3s ease;z-index:0}.profile:hover .hover-image{opacity:1}.more-info{position:relative;z-index:2;margin-top:10px}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Nikolaos Tsagkas. Template from <a href="https://github.com/alshedivat/al-folio/" target="_blank" rel="external nofollow noopener">al-folio</a>. Last updated: June 10, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-K8JK18X15P"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-K8JK18X15P");</script> </body> </html>