---
layout: about
title: about
permalink: /
subtitle: 
# PhD candidate @ <a href="http://web.inf.ed.ac.uk/ipab" target="_blank">University of Edinburgh</a>, <a href="https://www.edinburgh-robotics.org/" target="_blank">Edinburgh Centre for Robotics</a>

profile:
  align: right
  image: prof_pic_2_25.jpg # prof_pic_2_25
  image_circular: false # crops the image to make it circular
  more_info: 
# >
# <p>Informatics Forum,</p>
# <p>10 Crichton Street,</p>
# <p>Edinburgh, EH8 9AB</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

<!-- I am a PhD candidate, at the [School of Informatics - University of Edinburgh](http://web.inf.ed.ac.uk/ipab), sponsored by the [Edinburgh Centre for Robotics](https://www.edinburgh-robotics.org/). I am privileged to be advised by [Prof. Chris Xiaoxuan Lu](https://christopherlu.github.io/) (UCL) and [Prof. Oisin Mac Aodha](https://homepages.inf.ed.ac.uk/omacaod/) (UoE). My research interests revolve around the use of pre-trained visual representations in robot learning. -->

I am a PhD candidate at the [School of Informatics, University of Edinburgh](http://web.inf.ed.ac.uk/ipab), supported by the [Edinburgh Centre for Robotics](https://www.edinburgh-robotics.org/). I am fortunate to be advised by [Prof. Chris Xiaoxuan Lu](https://christopherlu.github.io/) (UCL) and [Prof. Oisin Mac Aodha](https://homepages.inf.ed.ac.uk/omacaod/) (UoE). My research focuses on leveraging pre-trained visual representations for robot learning.

Prior to my PhD, I earned an MSc in Artificial Intelligence with distinction in 2021 at the University of Edinburgh, under the supervision of [Prof. Chris Williams](https://homepages.inf.ed.ac.uk/ckiw/), where I worked on inference and learning for [generative capsule models](https://arxiv.org/abs/2209.03115). Before that, I spent a year as a Data Scientist at *Ernst & Young*. I hold a BSc and MSc in Electrical & Computer Engineering (2019) from the University of Patras, Greece, where I researched [real-time hand-gesture recognition](https://ieeexplore.ieee.org/document/8900709) using sEMG signals under the guidance of [Prof. A. Skodras](http://www.ece.upatras.gr/skodras/).

<!-- In November 2021, I completed with distinction the Artificial Intelligence master's programme at the University of Edinburgh, where I worked under the supervision of [Prof. Chris Williams](https://homepages.inf.ed.ac.uk/ckiw/) on the topic of inference and learning for [generative capsule models](https://arxiv.org/abs/2209.03115). Before that, I worked for a year as a Data Scientist at *Ernst & Young*. In August 2019, I was awarded the Diploma of Electrical and Computer Engineering from the University of Patras, in Greece (graduated 4th in my class out of 202 students ‚Äì GPA: 8.11 out of 10). I completed my ECE Diploma thesis under the supervision of [Prof. Athanassios Skodras](http://www.ece.upatras.gr/skodras/), researching [real-time hand-gesture recognition](https://ieeexplore.ieee.org/document/8900709) via sEMG signals. -->

<!-- :loudspeaker: <mark><b>News</b></mark>:
<details>
  <summary>Click here to read older news</summary>
  <p>This is the hidden content that appears when you click.</p>
</details> -->

<!-- :loudspeaker:  <mark><b>News</b></mark>: I will be presenting my work at [IROS'24](https://iros2024-abudhabi.org/) in Abu Dhabi, followed by the [BMVA Symposium: Robotics Foundation & World Models](https://www.bmva.org/meetings/24-10-30-RobotWorldModels.html) in London. **Come say hi if you're there!** -->

<!-- üö® -->

<div class="news-section">
  üì¢  <b>Latest News:</b>
  <ul>
    <li>
    <b>June 2025</b>: üèÜ A short version of <a href="../pvrobo/" target="_blank">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning has been accepted</a> <b>for oral and poster presentation</b> at the <a href="https://embodied-ai.org/cvpr2025/" target="_blank">Embodied AI Workshop</a> at CVPR 2025. See you in Nashville, Tennessee! üëã 
    </li>
    <li>
      <b>May 2025</b>: üìú New paper published on ArXiv! You can access it here: 
      <a href="https://arxiv.org/abs/2505.01179" target="_blank">Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings</a>.
    </li>
  </ul>
  <details>
    <summary style="font-weight: normal; "> View older news</summary>
    <ul>
        <li>
      <b>Feb 2025</b>: üìú New paper published on ArXiv! You can access it here: 
      <a href="https://arxiv.org/abs/2502.03270" target="_blank">When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning</a>.
    </li>
      <div style="text-align: center; position: relative; padding-bottom: 10px;">
        <span style="position: absolute; top: -10px; left: 50%; transform: translateX(-50%); background-color: #000000; color: #ffffff; padding: 0 5px;font-weight: bold;">2024</span>
        <hr style="border: 1px solid #000;">
      </div>
      <li>
        <b>Oct 2024</b>: üó£Ô∏è I will be presenting our paper, <a href="https://tsagkas.github.io/click2grasp/">Click to Grasp</a>, during the Robot Vision IV session from 09:00 to 10:00 on Fri 18 Oct at Room 4, IROS'24 in Abu Dhabi, UAE.
      </li>
      <li>
        <b>Sep 2024</b>: üó£Ô∏è Our paper, <a href="https://tsagkas.github.io/click2grasp/">Click to Grasp</a>, will be presented at the 
        <a href="https://www.bmva.org/meetings/24-10-30-RobotWorldModels.html" target="_blank">BMVA Symposium: Robotics Foundation & World Models</a> in London.
      </li>
      <li>
        <b>Aug 2024</b>: üìú New paper published on ArXiv! You can access it here: 
        <a href="https://arxiv.org/abs/2408.10123" target="_blank">Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</a>.
      </li>
      <li>
        <b>Jun 2024</b>: üèÜ Our paper, <a href="https://tsagkas.github.io/click2grasp/" target="_blank">Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors</a>, has been accepted at the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
      </li>
      <div style="text-align: center; position: relative; padding-bottom: 10px;">
        <span style="position: absolute; top: -10px; left: 50%; transform: translateX(-50%); background-color: #000000; color: #ffffff; padding: 0 5px;font-weight: bold;">2023</span>
        <hr style="border: 1px solid #000;">
      </div>
      <li>
        <b>May 2023</b>: üèÜ Our paper, <a href="https://tsagkas.github.io/vl-fields/" target="_blank">VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations</a>, has been accepted (spotlight) at the 
        <i>Workshop on Effective Representations, Abstractions, and Priors for Robot Learning</i>, ICRA 2023.
      </li>
    </ul>
  </details>
</div>

<style>
    .icon {
        color: #000; /* black color */
        font-size: 27px; /* larger size */
    }

  .news-section {
    background-color:rgb(250, 250, 250); /*rgb(241, 241, 241); /* Light gray background #f8f9fa*/
    border-left: 4px solidrgb(2, 2, 2); /* Blue accent border #007bff */
    padding: 10px 15px;
    margin: 20px 0;
    border-radius: 5px;
    font-size: 16px;
    box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.15); /* Soft shadow effect */
  }

  .news-section summary {
    font-weight: bold;
    cursor: grab;
    /* color: #007bff; Blue color matching links or black 111111*/
    color: #111111; 
  }

  .news-section ul {
    padding-left: 20px;
    margin-top: 10px;
  }

  .news-section li {
    margin-bottom: 5px;
  }
</style>

<!-- **Connect**:<br> -->
<a href="mailto:n.tsagkas@ed.ac.uk"><i class="fas fa-envelope icon"></i></a>
<a href="https://github.com/tsagkas"><i class="fab fa-github icon"></i></a>
<a href="https://scholar.google.com/citations?user=cZgkD_oAAAAJ"><i class="ai ai-google-scholar-square ai-24x icon"></i></a>
<a href="https://twitter.com/NikolasTsagkas"><i class="fa-brands fa-x-twitter icon"></i></a>
<a href="https://www.linkedin.com/in/nikolas-tsagkas/"><i class="fab fa-linkedin icon"></i></a>

<!-- Make sure you include Font Awesome CSS -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">



<!-- **Connect**:<br>
n (dot) tsagkas (at) ed.ac.uk<br>
<a href="https://github.com/tsagkas"><img src="https://image.flaticon.com/icons/svg/25/25231.svg" alt="GitHub" width="20" height="20"></a>
<a href="https://scholar.google.com/citations?user=cZgkD_oAAAAJ"><img src="https://image.flaticon.com/icons/svg/25/25239.svg" alt="Google Scholar" width="20" height="20"></a>
<a href="https://twitter.com/NikolasTsagkas"><img src="https://image.flaticon.com/icons/svg/25/25236.svg" alt="Twitter" width="20" height="20"></a>
<a href="https://www.linkedin.com/in/nikolas-tsagkas/"><img src="https://image.flaticon.com/icons/svg/25/25225.svg" alt="LinkedIn" width="20" height="20"></a>
 -->

<!-- **Connect**:<br>
n (dot) tsagkas (at) ed.ac.uk<br>
[[GitHub](https://github.com/tsagkas)] [[G. Scholar](https://scholar.google.com/citations?user=cZgkD_oAAAAJ)] [[X](https://twitter.com/NikolasTsagkas)] [[LinkedIn](https://www.linkedin.com/in/nikolas-tsagkas/)]<br> -->
<!-- **Code**: https://github.com/tsagkas<br>
**X**: https://twitter.com/NikolasTsagkas<br> -->